\documentclass[11pt]{article}

% --- Packages ---
\usepackage{mathpazo}        % Loads Palatino + math support
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm, amsfonts}
\usepackage{mathtools}       % For extra math tools
\usepackage{bm}              % Bold math symbols
\usepackage{enumitem}        % Better control over lists
\usepackage{geometry}        % Better margins
\geometry{margin=1in}

% --- Custom Commands ---
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\Xcal}{\mathcal{X}}  % Input space
\newcommand{\Ycal}{\mathcal{Y}}  % Output / label space
\newcommand{\Hcal}{\mathcal{H}}  % Hypothesis space
\newcommand{\Dcal}{\mathcal{D}}  % Distribution or dataset

\newcommand{\eps}{\varepsilon}
\newcommand{\del}{\partial}

\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{#1}}

\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}

% --- Theorem Environments ---
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{plain}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}

\theoremstyle{remark}
\newtheorem{remark}[definition]{Remark}
\newtheorem{example}[definition]{Example}

% --- Title Info ---
\title{Mathematics for Machine Learning}
\author{Leo Baker-Hytch}
\date{\today}

% --- Begin Document ---
\begin{document}

\maketitle
\tableofcontents
\vspace{1em}

\section{Vector spaces and subspaces}

\begin{definition}
    A \emph{vector space} over a field \( \F \) is a set \( V \) equipped with two operations: vector addition and
    scalar multiplication, satisfying the usual axioms.
\end{definition}

\begin{definition}
    A subset \( U \subseteq V \) is a \emph{subspace} of a vector space \( V \) if:
    \begin{itemize}
        \item \( 0 \in U \)
        \item \( u, w \in U \implies u + w \in U \)
        \item \( a \in \F, u \in U \implies a u \in U \)
    \end{itemize}
\end{definition}

\begin{example}
    The set of continuous real-valued functions \( C(\R) \) is a subspace of \( \R^\R \).
\end{example}

\section{Function spaces}

Let \( \R^\R \) denote the set of all functions from \( \R \) to \( \R \). We define:

\begin{itemize}
    \item \( C(\R) \): continuous functions
    \item \( C^1(\R) \): continuously differentiable functions
    \item \( C^\infty(\R) \): smooth functions
\end{itemize}

Clearly, \( C^\infty(\R) \subset C^1(\R) \subset C(\R) \subset \R^\R \).


\section{Sequence spaces}

Let \( \C^\N \) denote the set of all complex sequences.

\begin{itemize}
    \item \( c_0 \): sequences converging to zero
    \item \( \ell^\infty \): bounded sequences
    \item \( \ell^2 \): square-summable sequences
\end{itemize}

Then we have: \( c_0 \subset \ell^\infty \subset \C^\N \), and each is a subspace of the next.


\section{Matrices and linear maps}

\begin{definition}
    Let \( \mat{A} \in \R^{m \times n} \). The \emph{rank} of \( \mat{A} \), denoted \( \rank(\mat{A}) \), is the
    dimension of the column space of \( \mat{A} \).
\end{definition}

\begin{theorem}[Rank-Nullity Theorem]
    Let \( \mat{T}: V \to W \) be a linear transformation between finite-dimensional vector spaces. Then:
    \[
        \dim(\Ker(\mat{T})) + \dim(\Img(\mat{T})) = \dim(V)
    \]
\end{theorem}

\begin{proof}
    Sketch: Represent \( \mat{T} \) as a matrix. Then the null space corresponds to the solution set of \(
    \mat{A}\vect{x} = 0 \), and the image corresponds to the span of the columns. Apply Gaussian elimination to count
    free variables and pivot columns.
\end{proof}


\section{Machine learning notation}

In machine learning, we often work with datasets of input-output pairs \( (\vect{x}_i, y_i) \), where each input \(
\vect{x}_i \in \Xcal \subset \R^d \) and each label \( y_i \in \Ycal \subset \R \) or \( \Ycal = \{0,1\} \) for
classification.

\begin{definition}
    Let \( \Xcal \subset \R^d \) be the input space and \( \Ycal \) the output space. A \emph{hypothesis} is a
    function \( h: \Xcal \to \Ycal \).
\end{definition}

\begin{example}
    In linear regression, we model the hypothesis as a linear function:
    \[
        h(\vect{x}) = \vect{w}^\top \vect{x} + b,
    \]
    where \( \vect{w} \in \R^d \) is the weight vector and \( b \in \R \) is a bias term.
\end{example}

\begin{definition}
    Given a dataset \( \{(\vect{x}_i, y_i)\}_{i=1}^n \), the empirical risk under squared loss is:
    \[
        R(\vect{w}) = \frac{1}{n} \sum_{i=1}^n \left( h(\vect{x}_i) - y_i \right)^2.
    \]
    Minimizing this objective corresponds to least-squares regression.
\end{definition}

\end{document}